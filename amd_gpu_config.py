"""
AMD GPU é…ç½®æ–‡ä»¶
è®¾ç½® ROCm ç¯å¢ƒå˜é‡å’Œä¼˜åŒ–å‚æ•°
"""

import os
import sys


def setup_amd_gpu_environment():
    """è®¾ç½®AMD GPUç¯å¢ƒå˜é‡"""

    # ROCm åŸºç¡€è·¯å¾„
    rocm_path = os.environ.get("ROCM_PATH", "/opt/rocm")

    # è®¾ç½® ROCm ç¯å¢ƒå˜é‡
    env_vars = {
        "ROCM_PATH": rocm_path,
        "HIP_PATH": rocm_path,
        "HSA_PATH": rocm_path,
        "LD_LIBRARY_PATH": f"{rocm_path}/lib:{os.environ.get('LD_LIBRARY_PATH', '')}",
        "PATH": f"{rocm_path}/bin:{os.environ.get('PATH', '')}",
        # ROCm 6.2 æ€§èƒ½ä¼˜åŒ–
        "HSA_FORCE_FINE_GRAIN_PCIE": "1",
        "HIP_VISIBLE_DEVICES": "0",
        "ROCM_TARGET_LST": "gfx906;gfx908;gfx90a;gfx942;gfx1030;gfx1100",  # æ·»åŠ  gfx942 (MI300X)
        # PyTorch ROCm 6.2 ä¼˜åŒ–
        "PYTORCH_ROCM_ARCH": "gfx906;gfx908;gfx90a;gfx942;gfx1030;gfx1100",  # æ·»åŠ  gfx942
        "HIP_FORCE_DEV_KERNARG": "1",
        # vLLM ROCm ä¼˜åŒ–
        "VLLM_USE_ROCM": "1",
        "VLLM_USE_V1": "0",  # ç¦ç”¨ V1 API
        # å†…å­˜ä¼˜åŒ–
        "PYTORCH_HIP_ALLOC_CONF": "max_split_size_mb:128",
        "HIP_LAUNCH_BLOCKING": "0",
        # è°ƒè¯•é€‰é¡¹ï¼ˆå¯é€‰ï¼‰
        # 'HIP_PRINT_STATS': '1',
        # 'AMD_LOG_LEVEL': '3',
    }

    # åº”ç”¨ç¯å¢ƒå˜é‡
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")

    # éªŒè¯ ROCm å®‰è£…
    try:
        import subprocess

        result = subprocess.run(
            ["rocm-smi"], capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            print("âœ… ROCm å®‰è£…éªŒè¯æˆåŠŸ")
            print("GPU ä¿¡æ¯:")
            print(result.stdout)
        else:
            print("âš ï¸  ROCm éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®‰è£…")
    except (subprocess.TimeoutExpired, FileNotFoundError) as e:
        print(f"âš ï¸  æ— æ³•éªŒè¯ ROCm å®‰è£…: {e}")

    return True


def check_amd_gpu_compatibility():
    """æ£€æŸ¥AMD GPUå…¼å®¹æ€§"""
    try:
        import torch

        print("ğŸ” æ£€æŸ¥ PyTorch ROCm æ”¯æŒ...")
        print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"ROCm å¯ç”¨: {torch.cuda.is_available()}")

        if torch.cuda.is_available():
            print(f"GPU è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"è®¾å¤‡ {i}: {torch.cuda.get_device_name(i)}")

            # æµ‹è¯•ç®€å•çš„ GPU æ“ä½œ
            try:
                x = torch.randn(100, 100).cuda()
                y = torch.mm(x, x.t())
                print("âœ… GPU è®¡ç®—æµ‹è¯•æˆåŠŸ")

                # æ¸…ç†æµ‹è¯•å¼ é‡
                del x, y
                torch.cuda.empty_cache()

            except Exception as e:
                print(f"âŒ GPU è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
                return False
        else:
            print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°å¯ç”¨çš„ GPU")
            return False

    except ImportError as e:
        print(f"âŒ PyTorch å¯¼å…¥å¤±è´¥: {e}")
        return False

    return True


def optimize_for_txagent():
    """ä¸º TxAgent ä¼˜åŒ– AMD GPU è®¾ç½®"""

    # TxAgent ç‰¹å®šä¼˜åŒ–
    txagent_env = {
        # æ¨¡å‹åŠ è½½ä¼˜åŒ–
        "TRANSFORMERS_CACHE": os.path.expanduser("~/.cache/huggingface/transformers"),
        "HF_HOME": os.path.expanduser("~/.cache/huggingface"),
        # å†…å­˜ç®¡ç†
        "TOKENIZERS_PARALLELISM": "false",
        "MKL_THREADING_LAYER": "GNU",
        # vLLM ä¼˜åŒ–
        "VLLM_ATTENTION_BACKEND": "ROCM_FLASH",
        "VLLM_WORKER_MULTIPROC_METHOD": "spawn",
    }

    for key, value in txagent_env.items():
        os.environ[key] = value
        print(f"TxAgent ä¼˜åŒ–: {key}={value}")


def main():
    """ä¸»å‡½æ•°ï¼šè®¾ç½®å®Œæ•´çš„ AMD GPU ç¯å¢ƒ"""
    print("ğŸš€ é…ç½® TxAgent AMD GPU ç¯å¢ƒ...")

    # è®¾ç½®åŸºç¡€ç¯å¢ƒ
    setup_amd_gpu_environment()

    # TxAgent ç‰¹å®šä¼˜åŒ–
    optimize_for_txagent()

    # å…¼å®¹æ€§æ£€æŸ¥
    if check_amd_gpu_compatibility():
        print("ğŸ‰ AMD GPU ç¯å¢ƒé…ç½®æˆåŠŸï¼")
        return True
    else:
        print("âŒ AMD GPU ç¯å¢ƒé…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
