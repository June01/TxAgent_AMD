"""
GPU å…¼å®¹æ€§å·¥å…·æ¨¡å—
æ”¯æŒ NVIDIA CUDA å’Œ AMD ROCm GPU
"""

import torch
import gc
import os
import logging

logger = logging.getLogger(__name__)


class GPUManager:
    """GPU ç®¡ç†å™¨ï¼Œè‡ªåŠ¨æ£€æµ‹å’Œç®¡ç† CUDA/ROCm GPU"""
    
    def __init__(self):
        self.device_type = self._detect_device_type()
        self.device_count = self._get_device_count()
        self.current_device = self._get_current_device()
        
        logger.info(f"æ£€æµ‹åˆ° GPU ç±»å‹: {self.device_type}")
        logger.info(f"å¯ç”¨è®¾å¤‡æ•°é‡: {self.device_count}")
        
    def _detect_device_type(self):
        """æ£€æµ‹GPUç±»å‹"""
        if torch.cuda.is_available():
            # æ£€æŸ¥æ˜¯å¦ä¸º AMD GPU (ROCm)
            try:
                device_name = torch.cuda.get_device_name(0).lower()
                if 'amd' in device_name or 'radeon' in device_name or 'gfx' in device_name:
                    return 'rocm'
                else:
                    return 'cuda'
            except:
                return 'cuda'  # é»˜è®¤å‡è®¾ä¸º CUDA
        else:
            return 'cpu'
    
    def _get_device_count(self):
        """è·å–è®¾å¤‡æ•°é‡"""
        if self.device_type in ['cuda', 'rocm']:
            return torch.cuda.device_count()
        return 0
    
    def _get_current_device(self):
        """è·å–å½“å‰è®¾å¤‡"""
        if self.device_type in ['cuda', 'rocm']:
            return torch.cuda.current_device()
        return None
    
    def empty_cache(self):
        """æ¸…ç©ºGPUç¼“å­˜"""
        try:
            if self.device_type in ['cuda', 'rocm']:
                torch.cuda.empty_cache()
                logger.debug(f"å·²æ¸…ç©º {self.device_type.upper()} ç¼“å­˜")
            gc.collect()
        except Exception as e:
            logger.warning(f"æ¸…ç©ºGPUç¼“å­˜æ—¶å‡ºé”™: {e}")
    
    def get_memory_info(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if self.device_type in ['cuda', 'rocm']:
            try:
                allocated = torch.cuda.memory_allocated()
                reserved = torch.cuda.memory_reserved()
                return {
                    'allocated': allocated,
                    'reserved': reserved,
                    'allocated_gb': allocated / 1024**3,
                    'reserved_gb': reserved / 1024**3
                }
            except Exception as e:
                logger.warning(f"è·å–GPUå†…å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                return None
        return None
    
    def set_device(self, device_id=0):
        """è®¾ç½®å½“å‰è®¾å¤‡"""
        if self.device_type in ['cuda', 'rocm'] and device_id < self.device_count:
            torch.cuda.set_device(device_id)
            self.current_device = device_id
            logger.info(f"åˆ‡æ¢åˆ°è®¾å¤‡ {device_id}")
        else:
            logger.warning(f"æ— æ³•åˆ‡æ¢åˆ°è®¾å¤‡ {device_id}")
    
    def get_device_name(self, device_id=0):
        """è·å–è®¾å¤‡åç§°"""
        if self.device_type in ['cuda', 'rocm'] and device_id < self.device_count:
            try:
                return torch.cuda.get_device_name(device_id)
            except Exception as e:
                logger.warning(f"è·å–è®¾å¤‡åç§°æ—¶å‡ºé”™: {e}")
                return f"Unknown {self.device_type.upper()} Device"
        return "CPU"
    
    def is_available(self):
        """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
        return self.device_type in ['cuda', 'rocm'] and torch.cuda.is_available()
    
    def get_device_properties(self, device_id=0):
        """è·å–è®¾å¤‡å±æ€§"""
        if self.device_type in ['cuda', 'rocm'] and device_id < self.device_count:
            try:
                props = torch.cuda.get_device_properties(device_id)
                return {
                    'name': props.name,
                    'major': props.major,
                    'minor': props.minor,
                    'total_memory': props.total_memory,
                    'total_memory_gb': props.total_memory / 1024**3,
                    'multi_processor_count': props.multi_processor_count
                }
            except Exception as e:
                logger.warning(f"è·å–è®¾å¤‡å±æ€§æ—¶å‡ºé”™: {e}")
                return None
        return None
    
    def optimize_for_inference(self):
        """ä¸ºæ¨ç†ä¼˜åŒ–GPUè®¾ç½®"""
        if self.device_type == 'rocm':
            # AMD ROCm ç‰¹å®šä¼˜åŒ–
            os.environ.setdefault('HSA_FORCE_FINE_GRAIN_PCIE', '1')
            os.environ.setdefault('HIP_VISIBLE_DEVICES', '0')
            logger.info("åº”ç”¨ ROCm æ¨ç†ä¼˜åŒ–è®¾ç½®")
        elif self.device_type == 'cuda':
            # NVIDIA CUDA ç‰¹å®šä¼˜åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            logger.info("åº”ç”¨ CUDA æ¨ç†ä¼˜åŒ–è®¾ç½®")
    
    def print_info(self):
        """æ‰“å°GPUä¿¡æ¯"""
        print(f"ğŸ”§ GPU ç®¡ç†å™¨ä¿¡æ¯:")
        print(f"   è®¾å¤‡ç±»å‹: {self.device_type.upper()}")
        print(f"   è®¾å¤‡æ•°é‡: {self.device_count}")
        print(f"   å½“å‰è®¾å¤‡: {self.current_device}")
        
        if self.is_available():
            for i in range(self.device_count):
                name = self.get_device_name(i)
                props = self.get_device_properties(i)
                print(f"   è®¾å¤‡ {i}: {name}")
                if props:
                    print(f"     æ€»å†…å­˜: {props['total_memory_gb']:.1f} GB")
                    print(f"     è®¡ç®—èƒ½åŠ›: {props['major']}.{props['minor']}")
        else:
            print("   âš ï¸  æ²¡æœ‰å¯ç”¨çš„GPUè®¾å¤‡")


# å…¨å±€GPUç®¡ç†å™¨å®ä¾‹
gpu_manager = GPUManager()


def empty_cache():
    """æ¸…ç©ºGPUç¼“å­˜çš„ä¾¿æ·å‡½æ•°"""
    gpu_manager.empty_cache()


def get_device():
    """è·å–æ¨èçš„è®¾å¤‡"""
    if gpu_manager.is_available():
        return f"cuda:{gpu_manager.current_device}"
    return "cpu"


def is_gpu_available():
    """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
    return gpu_manager.is_available()


def optimize_for_inference():
    """ä¸ºæ¨ç†ä¼˜åŒ–GPUè®¾ç½®"""
    gpu_manager.optimize_for_inference()


def print_gpu_info():
    """æ‰“å°GPUä¿¡æ¯"""
    gpu_manager.print_info()


# è‡ªåŠ¨ä¼˜åŒ–è®¾ç½®
if gpu_manager.is_available():
    optimize_for_inference()
